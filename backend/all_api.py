import os
import requests
from main_content_extractor import MainContentExtractor
from datetime import datetime, timedelta
from dotenv import load_dotenv
import json
import re
from urllib.parse import urlparse, urlunparse
import hashlib
from transformers import pipeline

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è ---
load_dotenv()
POLYGON_API_KEY = os.getenv("POLYGON_IO_API")
ALPHA_VANTAGE_API_KEY = os.getenv("ALPHA_VANTAGE_API")
FMP_API_KEY = os.getenv("FINANCIAL_MODELING_PREP_API")
NEWS_API_KEY = os.getenv("NEWS_API")

MAX_TOPIC_LENGTH = 100
try:
    ner_pipeline = pipeline(
        "ner",
        model="Babelscape/wikineural-multilingual-ner",
        aggregation_strategy="simple",
        device=-1  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ CPU
    )
    print("‚úÖ NER –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NER –º–æ–¥–µ–ª–∏: {e}")
    ner_pipeline = None


# --- –î–∞—Ç—ã —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ ---
start_date_str = "2025-10-03"
end_date_str = "2025-10-04"

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def has_duplicate_words(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥."""
    words = text.split()
    if len(words) < 2:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–ª–Ω–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—Ä–∞–∑—ã
    text_lower = text.lower()
    half_len = len(text) // 2
    if len(text) > 3 and text_lower[:half_len] == text_lower[half_len:half_len * 2]:
        return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–ª–æ–≤–∞
    for i in range(len(words) - 1):
        if words[i].lower() == words[i + 1].lower():
            return True

    return False

def remove_nested_duplicates(nested_list):
    seen = set()
    unique_list = []
    for sublist in nested_list:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤ –∫–æ—Ä—Ç–µ–∂ –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        tuple_sublist = tuple(sublist)
        if tuple_sublist not in seen:
            seen.add(tuple_sublist)
            unique_list.append(sublist)
    return unique_list

def to_final_format(item: dict) -> dict:
    final = {
        "url": item.get("source_url"),
        "canonical_url": item.get("canonical_url"),
        "domain": item.get("source_domain"),
        "published_time": item.get("published_at_utc") or "",
        "crawl_time": datetime.now().isoformat(),
        "last_modified": item.get("published_at_utc") or "",
        "title": item.get("title"),
        "lead": item.get("summary"),
        "text": item.get("content"),
        "language": item.get("language"),
        "content_hash": item.get("content_hash"),
        "entities": item.get("topics", []),
        "tickers": item.get("tickers", [])
    }
    return final
def generate_content_hash(text: str) -> str:
    if not text:
        return ""
    text = text.strip().encode("utf-8")
    return hashlib.sha256(text).hexdigest()
def get_canonical_url(url: str) -> str:
    if not url:
        return ""

    parsed = urlparse(url.strip())
    scheme = parsed.scheme or "https"
    netloc = parsed.netloc.lower().lstrip("www.")
    path = parsed.path.rstrip("/")

    canonical = urlunparse((scheme, netloc, path, "", "", ""))
    return canonical

def get_source_domain(url: str) -> str:
    if not url:
        return ""
    parsed = urlparse(url)
    return parsed.netloc.lower().lstrip("www.")


def extract_entities_universal(text: str):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è NER —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ + –∞–Ω–≥–ª–∏–π—Å–∫–∏–π."""
    if not text or len(text.strip()) < 10:
        return []

    entities = []

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ pipeline –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
        if 'ner_pipeline' not in globals() or ner_pipeline is None:
            print("‚ö†Ô∏è NER pipeline –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
            return []

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞ (wikineural —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–æ–∫–µ–Ω–∞–º–∏, –Ω–µ —Å–∏–º–≤–æ–ª–∞–º–∏)
        text_sample = text[:2000]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç

        print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª–∏–Ω–æ–π {len(text_sample)} —Å–∏–º–≤–æ–ª–æ–≤...")
        ner_results = ner_pipeline(text_sample)

        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(ner_results)} —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NER")

        for entity in ner_results:
            if entity['entity_group'] in ['ORG', 'PER', 'LOC'] and entity['score'] > 0.6:
                clean_entity = entity['word'].strip().replace('‚ñÅ', ' ').strip()
                if 3 <= len(clean_entity) <= MAX_TOPIC_LENGTH:
                    entities.append(clean_entity)
                    print(f"  ‚úÖ {clean_entity} ({entity['entity_group']}, {entity['score']:.2f})")

    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ NER: {type(e).__name__}: {str(e)}")
        import traceback
        traceback.print_exc()

    # Regex-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–∞–∫ fallback
    patterns = [
        r'\b([A-Z–ê-–Ø–Å][a-zA-Z–∞-—è—ë–ê-–Ø–Å&\s]+(?:Inc\.|Corp\.|Ltd\.|LLC|Co\.|Corporation|Limited|Group|Holdings))',
        r'\b(–û–û–û|–ü–ê–û|–ê–û|–ó–ê–û|–ò–ü|–ì–ö)\s+[¬´"]?([–ê-–Ø–Å][–∞-—è—ë–ê-–Ø–Å\s\-]+)[¬ª"]?',
    ]

    for pattern in patterns:
        matches = re.finditer(pattern, text[:2000])
        for match in matches:
            entity = match.group(0).strip()
            if 3 <= len(entity) <= MAX_TOPIC_LENGTH:
                entities.append(entity)

    # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    entities = list(dict.fromkeys(entities))  # –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫
    entities = [e for e in entities if not has_duplicate_words(e)]

    print(f"üìã –ò—Ç–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(entities)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π")
    return entities

def parser_html(url):
    try:
        response = requests.get(url)
        response.raise_for_status()  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Å–ø–µ—à–Ω—ã–π —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
        response.encoding = 'utf-8'
        content = response.text

        # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        return MainContentExtractor.extract(content, output_format="text")
    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ URL {url}: {e}")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ URL {url}: {e}")
    return ""  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

def normalize_date_str(date_str):
    year, month, day = date_str.split("-")
    return f"{year}-{month.zfill(2)}-{day.zfill(2)}"

def parse_date(date_str):
    formats = ("%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%d %H:%M:%S", "%Y-%m-%d")
    for fmt in formats:
        try:
            return datetime.strptime(date_str, fmt)
        except:
            continue
    return None

# --- Polygon.io ---
def get_polygon_news_all(start_date, end_date, fetch_full_text=True):
    news_list = []
    seen_ids = set()  # —á—Ç–æ–±—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
    start_fixed = normalize_date_str(start_date)
    end_fixed = normalize_date_str(end_date)
    offset = 0
    page_limit = 100
    max_pages = 50  # –º–∞–∫—Å–∏–º—É–º 5000 –Ω–æ–≤–æ—Å—Ç–µ–π (50 * 100)

    for page in range(max_pages):
        print(f"–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1} –Ω–æ–≤–æ—Å—Ç–µ–π Polygon.io...")
        url = (
            f"https://api.polygon.io/v2/reference/news?"
            f"published_utc.gte={start_fixed}&published_utc.lte={end_fixed}"
            f"&limit={page_limit}&offset={offset}&apiKey={POLYGON_API_KEY}"
        )

        try:
            resp = requests.get(url, timeout=15)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ Polygon.io: {e}")
            break

        results = data.get("results", [])
        if not results:
            print("–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –ª–µ–Ω—Ç—ã Polygon.io.")
            break

        new_items = 0
        for item in results:
            print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏:", item.get("title", "")[:60], " –æ—Ç Polygon.io")
            article_id = item.get("id") or item.get("article_url")
            if article_id in seen_ids:
                continue
            seen_ids.add(article_id)

            content = item.get("content", "")
            if fetch_full_text and (not content or "[+" in content) and len(content) < 200:
                full_text = parser_html(item.get("article_url"))
                if full_text:
                    content = full_text
            if content == "":
                continue
            topics = item.get("keywords", [])
            # topics += extract_entities_en(content)
            # topics = remove_nested_duplicates(topics)
            # topics = [topic for topic in topics if len(topic) <= MAX_TOPIC_LENGTH and not has_duplicate_words(topic)]
            news_list.append(to_final_format({
                "title": item.get("title"),
                "summary": item.get("description"),
                "content": content,
                "author": item.get("author"),
                "published_at_utc": item.get("published_utc"),
                "source_domain": get_source_domain(item.get("article_url")),
                "source_url": item.get("article_url"),
                "canonical_url": get_canonical_url(item.get("article_url")),
                "tickers": item.get("tickers", []),
                "topics": topics,
                "language": "en",
                "content_hash": generate_content_hash(content)
            }))
            new_items += 1

        print(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {new_items} –Ω–æ–≤–æ—Å—Ç–µ–π")

        if new_items < page_limit:
            print("–ü–æ—Ö–æ–∂–µ, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.")
            break

        offset += page_limit

    print(f"Polygon.io: –≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return news_list

# --- Alpha Vantage ---
def get_alpha_vantage_news_all(start_date, end_date, topics="finance,business,technology", limit=100, fetch_full_text=True):
    news_list = []
    start_norm = normalize_date_str(start_date)
    end_norm = normalize_date_str(end_date)
    time_from = datetime.strptime(start_norm, "%Y-%m-%d").strftime("%Y%m%dT0000")
    time_to = datetime.strptime(end_norm, "%Y-%m-%d").strftime("%Y%m%dT2359")
    url = f"https://www.alphavantage.co/query?function=NEWS_SENTIMENT&topics={topics}&time_from={time_from}&time_to={time_to}&limit={limit}&sort=LATEST&apikey={ALPHA_VANTAGE_API_KEY}"
    try:
        resp = requests.get(url, timeout=20)
        resp.raise_for_status()
        data = resp.json()
        feed = data.get("feed", [])
        for item in feed:
            print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏:", item.get("title", "")[:60], " –æ—Ç Alpha Vantage")
            title = item.get("title", "")
            summary = item.get("summary", "")
            content = item.get("content", "")
            source_url = item.get("url", "")
            if fetch_full_text and source_url and (not content or "[+" in content or len(content)<200):
                full_text = parser_html(source_url)
                if full_text:
                    content = full_text
            time_str = item.get("time_published")
            pub = None
            for fmt in ("%Y%m%dT%H%M%S", "%Y%m%dT%H%M"):
                try:
                    pub = datetime.strptime(time_str, fmt)
                    break
                except:
                    continue
            if not pub:
                continue
            if content == "":
                continue
            topics_raw = item.get("topics", [])
            topics = [
                t["topic"] if isinstance(t, dict) and "topic" in t else t
                for t in topics_raw
            ]
            news_list.append(to_final_format({
                "api_source": "Alpha Vantage",
                "title": title,
                "summary": summary,
                "content": content,
                "author": ", ".join(item.get("authors", [])),
                "published_at_utc": pub.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "source_domain": get_source_domain(source_url),
                "source_url": source_url,
                "canonical_url": get_canonical_url(source_url),
                "tickers": [t.get("ticker") for t in item.get("ticker_sentiment", []) if t.get("ticker")] if item.get("ticker_sentiment") else [],
                "topics": topics,
                "language": "en",
                "content_hash": generate_content_hash(content)
            }))
        print(f"Alpha Vantage: –Ω–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ Alpha Vantage: {e}")
    return news_list

# --- FMP ---
def get_fmp_news_all(start_date, end_date, fetch_full_text=True):
    news_list = []
    seen_titles = set()  # –ß—Ç–æ–±—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
    start_norm = normalize_date_str(start_date)
    end_norm = normalize_date_str(end_date)
    start_dt = datetime.strptime(start_norm, "%Y-%m-%d").date()
    end_dt = datetime.strptime(end_norm, "%Y-%m-%d").date()
    page = 0
    limit = 100
    max_pages = 50  # –º–∞–∫—Å–∏–º—É–º 5000 –Ω–æ–≤–æ—Å—Ç–µ–π

    while page < max_pages:
        print(f"–ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page + 1} –Ω–æ–≤–æ—Å—Ç–µ–π FMP...")
        url = f"https://financialmodelingprep.com/stable/fmp-articles?page={page}&limit={limit}&apikey={FMP_API_KEY}"
        try:
            resp = requests.get(url, timeout=15)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ FMP (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}): {e}")
            break

        if not data or not isinstance(data, list):
            print("FMP: –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö")
            break

        count_added = 0
        for item in data:
            print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏:", item.get("title", "")[:60], " –æ—Ç FMP")
            pub_dt = parse_date(item.get("publishedDate") or item.get("date"))
            if not pub_dt or not (start_dt <= pub_dt.date() <= end_dt):
                continue

            title = item.get("title", "").strip()
            if not title or title in seen_titles:
                continue
            seen_titles.add(title)

            content = strip_html(item.get("text") or item.get("content") or "")
            if fetch_full_text and item.get("link") and len(content) < 200:
                full_text = parser_html(item.get("link"))
                if full_text:
                    content = full_text
            if content == "":
                continue
            topics = []
            topics += extract_entities_universal(content)
            topics = remove_nested_duplicates(topics)
            topics = [topic for topic in topics if len(topic) <= MAX_TOPIC_LENGTH and not has_duplicate_words(topic)]
            news_list.append(to_final_format({
                "api_source": "Financial Modeling Prep",
                "title": title,
                "summary": None,
                "content": content,
                "author": item.get("author", ""),
                "published_at_utc": pub_dt.strftime("%Y-%m-%dT%H:%M:%SZ"),
                "source_domain": get_source_domain(item.get("link") or item.get("url")),
                "source_url": item.get("link") or item.get("url"),
                "canonical_url": get_canonical_url(item.get("link") or item.get("url")),
                "tickers": [item.get("tickers")] if item.get("tickers") else [],
                "topics": topics,
                "language": "en",
                "content_hash": generate_content_hash(content)
            }))
            count_added += 1

        print(f"‚úÖ FMP —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page + 1}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {count_added} –Ω–æ–≤–æ—Å—Ç–µ–π (–≤—Å–µ–≥–æ {len(news_list)})")

        # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ –º–µ–Ω—å—à–µ, —á–µ–º –ª–∏–º–∏—Ç ‚Äî —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü
        if count_added < limit:
            print("–ü–æ—Ö–æ–∂–µ, –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –ª–µ–Ω—Ç—ã FMP.")
            break

        page += 1

    print(f"FMP: –≤—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    return news_list

# --- NewsAPI ---
def get_newsapi_news(start_date, end_date, fetch_full_text=True, lan = 'en'):
    """–ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç NewsAPI —Å–æ –≤—Å–µ–º–∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –ø–æ–ª—è–º–∏."""
    print("–ó–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π –æ—Ç NewsAPI...")
    news_list = []

    try:
        start_norm = normalize_date_str(start_date)
        end_norm = normalize_date_str(end_date)
        start_dt = datetime.strptime(start_norm, "%Y-%m-%d")
        end_dt = datetime.strptime(end_norm, "%Y-%m-%d")
    except ValueError as e:
        print(f"–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã: {e}")
        return []

    today = datetime.now()
    one_month_ago = today - timedelta(days=30)

    if start_dt < one_month_ago:
        original_start = start_norm
        start_norm = one_month_ago.strftime("%Y-%m-%d")
        print(f"‚ö†Ô∏è  NewsAPI –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω: start_date —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω —Å {original_start} –Ω–∞ {start_norm}")

    if end_dt > today:
        end_norm = today.strftime("%Y-%m-%d")

    url = 'https://newsapi.org/v2/everything'
    params = {
        'q': 'finance OR business OR stock OR earnings OR economy',
        'from': start_norm,
        'to': end_norm,
        'sortBy': 'publishedAt',
        'language': lan,
        'pageSize': 100,
        'apiKey': NEWS_API_KEY
    }

    try:
        response = requests.get(url, params=params, timeout=15)

        if response.status_code == 426:
            print("NewsAPI 426: –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ä—à–µ 1 –º–µ—Å—è—Ü–∞.")
            return []

        response.raise_for_status()
        data = response.json()

        for item in data.get("articles", []):
            title = (item.get("title") or "").strip()
            source_url = (item.get("url") or "").strip()
            if not title or not source_url:
                continue
            print("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–∏:", item.get("title", "")[:60], " –æ—Ç NewsAPI")
            content = ""
            # --- –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ "[+... chars]" ---
            if fetch_full_text:
                print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏: {title[:60]}...")
                full_text = parser_html(source_url)
                if full_text:
                    content = full_text

            # –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç–æ—Ç—ã
            if not content.strip():
                continue
            topics = []
            topics += extract_entities_universal(content)
            topics = remove_nested_duplicates(topics)
            topics = [topic for topic in topics if len(topic) <= MAX_TOPIC_LENGTH and not has_duplicate_words(topic)]
            news_list.append(to_final_format({
                    "api_source": "NewsAPI",
                    "title": item.get("title", ""),
                    "summary": item.get("description", ""),
                    "content": content.strip(),
                    "author": item.get("author"),
                    "published_at_utc": item.get("publishedAt"),
                    "source_domain": get_source_domain(item.get("url")),
                    "source_url": item.get("url"),
                    "canonical_url": get_canonical_url(item.get("url")),
                    "tickers": [],
                    "topics": topics,
                    "language": lan,
                    "content_hash": generate_content_hash(content)
                }))
        print(f"NewsAPI: –ø–æ–ª—É—á–µ–Ω–æ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π")
    except requests.exceptions.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ NewsAPI: {e}")

    return news_list

def strip_html(html_str):
    if not html_str:
        return ""
    clean = re.sub(r'<[^>]+>', '', html_str)
    clean = clean.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    return re.sub(r'\s+', ' ', clean).strip()


# from datetime import datetime, timedelta
#
#
# def fetch_news_by_day_range(api_func, start_date_str, end_date_str, **kwargs):
#     """Fetches news day by day, working backwards from end_date to start_date."""
#     all_results = []
#
#     try:
#         end_date = datetime.strptime(end_date_str, "%Y-%m-%d")
#         start_date = datetime.strptime(start_date_str, "%Y-%m-%d")
#     except ValueError as e:
#         print(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã: {e}")
#         return []
#
#     current_end = end_date
#
#     while current_end >= start_date:
#         current_start = current_end - timedelta(days=1)
#
#         # –ù–µ —É—Ö–æ–¥–∏–º –∑–∞ –ø—Ä–µ–¥–µ–ª—ã start_date
#         if current_start < start_date:
#             current_start = start_date
#
#         start_str = current_start.strftime("%Y-%m-%d")
#         end_str = current_end.strftime("%Y-%m-%d")
#
#         print(f"\nüìÖ –ó–∞–ø—Ä–æ—Å –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {start_str} ‚Üí {end_str}")
#
#         try:
#             daily_news = api_func(start_str, end_str, **kwargs)
#
#             if not daily_news:
#                 print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è {start_str} ‚Üí {end_str}, –ø—Ä–µ—Ä—ã–≤–∞–µ–º")
#                 break
#
#             all_results.extend(daily_news)
#             print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(daily_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å")
#
#         except Exception as e:
#             print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {start_str} ‚Üí {end_str}: {e}")
#             break
#
#         # –°–¥–≤–∏–≥–∞–µ–º—Å—è –Ω–∞ –¥–µ–Ω—å –Ω–∞–∑–∞–¥
#         current_end = current_start - timedelta(days=1)
#
#         if current_end < start_date:
#             break
#
#     return all_results
#
#
# if __name__ == "__main__":
#     all_news = []
#
#     if POLYGON_API_KEY:
#         all_news.extend(fetch_news_by_day_range(
#             get_polygon_news_all, start_date_str, end_date_str
#         ))
#
#     if ALPHA_VANTAGE_API_KEY:
#         all_news.extend(fetch_news_by_day_range(
#             get_alpha_vantage_news_all, start_date_str, end_date_str
#         ))
#
#     if FMP_API_KEY:
#         all_news.extend(fetch_news_by_day_range(
#             get_fmp_news_all, start_date_str, end_date_str
#         ))
#
#     if NEWS_API_KEY:
#         all_news.extend(fetch_news_by_day_range(
#             get_newsapi_news, start_date_str, end_date_str
#         ))
#         all_news.extend(fetch_news_by_day_range(
#             get_newsapi_news, start_date_str, end_date_str, lan='ru'
#         ))
#
#     all_news.sort(key=lambda x: x['published_time'], reverse=True)
#     print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
#
#     if all_news:
#         print(json.dumps(all_news[0], indent=4, ensure_ascii=False))
#
#     with open("all_news.json", "w", encoding="utf-8") as f:
#         json.dump(all_news, f, indent=4, ensure_ascii=False)

# --- –ì–ª–∞–≤–Ω—ã–π —Å–±–æ—Ä ---
if __name__ == "__main__":
    all_news = []
    if POLYGON_API_KEY:
        all_news.extend(get_polygon_news_all(start_date_str, end_date_str))
    if ALPHA_VANTAGE_API_KEY:
        all_news.extend(get_alpha_vantage_news_all(start_date_str, end_date_str))
    if FMP_API_KEY:
        all_news.extend(get_fmp_news_all(start_date_str, end_date_str))
    if NEWS_API_KEY:
        all_news.extend(get_newsapi_news(start_date_str, end_date_str))
        all_news.extend(get_newsapi_news(start_date_str, end_date_str, lan='ru'))

    all_news.sort(key=lambda x: x['published_time'], reverse=True)
    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
    if all_news:
        print(json.dumps(all_news[0], indent=4, ensure_ascii=False))
    with open("all_api_news_last_24.json", "w", encoding="utf-8") as f:
        json.dump(all_news, f, indent=4, ensure_ascii=False)